{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o6zeUmyeRL7K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')"
      ],
      "metadata": {
        "id": "FFZHdGQJRWUm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json(json_path):\n",
        "    \"\"\"Parse the JSON file and return the relevant configuration.\"\"\"\n",
        "    with open(json_path, 'r') as file:\n",
        "        config = json.load(file)\n",
        "    return config"
      ],
      "metadata": {
        "id": "WcdrvD_ORbeF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(csv_path, feature_config):\n",
        "    \"\"\"Load the CSV and preprocess the data based on feature handling settings.\"\"\"\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    for feature, settings in feature_config.items():\n",
        "        if settings['is_selected']:\n",
        "            if settings['feature_variable_type'] == 'numerical':\n",
        "                if settings['feature_details']['missing_values'] == 'Impute':\n",
        "                    impute_strategy = settings['feature_details'].get('impute_with', 'mean')\n",
        "                    impute_value = settings['feature_details'].get('impute_value', 0)\n",
        "\n",
        "                    if impute_strategy == 'Average of values':\n",
        "                        imputer = SimpleImputer(strategy='mean')\n",
        "                    elif impute_strategy == 'custom':\n",
        "                        imputer = SimpleImputer(strategy='constant', fill_value=impute_value)\n",
        "\n",
        "                    data[feature] = imputer.fit_transform(data[[feature]])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "_T5ZRbfZRflb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_features(data, target, method, config=None):\n",
        "    \"\"\"Reduce features based on the specified method.\"\"\"\n",
        "    if method == 'PCA':\n",
        "        n_components = config.get('num_of_features_to_keep', 2)\n",
        "        pca = PCA(n_components=n_components)\n",
        "        reduced_data = pca.fit_transform(data)\n",
        "        return pd.DataFrame(reduced_data)\n",
        "    elif method == 'Tree-based':\n",
        "        model = RandomForestRegressor(n_estimators=config.get('num_of_trees', 5))\n",
        "        model.fit(data, target)\n",
        "        importances = model.feature_importances_\n",
        "        important_indices = np.argsort(importances)[-config.get('num_of_features_to_keep', 4):]\n",
        "        return data.iloc[:, important_indices]\n",
        "    elif method == 'Corr with Target':\n",
        "        correlations = data.corrwith(target)\n",
        "        important_features = correlations.abs().sort_values(ascending=False).head(config.get('num_of_features_to_keep', 4)).index\n",
        "        return data[important_features]\n",
        "    elif method == 'No Reduction':\n",
        "        return data\n",
        "    else:\n",
        "        raise ValueError(\"Unknown reduction method\")"
      ],
      "metadata": {
        "id": "9OsAbidvRkRH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # File paths\n",
        "    json_path = 'algoparams_from_ui.json'\n",
        "    csv_path = 'iris.csv'\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        config = parse_json(json_path)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing JSON file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Check if necessary keys exist in JSON\n",
        "    if 'design_state_data' not in config:\n",
        "        logging.error(\"'design_state_data' key is missing in the JSON.\")\n",
        "        return\n",
        "\n",
        "    design_state_data = config['design_state_data']\n",
        "\n",
        "    if 'session_info' not in design_state_data or 'dataset' not in design_state_data['session_info']:\n",
        "        logging.error(\"Necessary keys like 'session_info' or 'dataset' are missing in 'design_state_data'.\")\n",
        "        return\n",
        "\n",
        "    if 'feature_handling' not in design_state_data:\n",
        "        logging.error(\"'feature_handling' key is missing in 'design_state_data'.\")\n",
        "        return\n",
        "\n",
        "    # Load and preprocess data\n",
        "    dataset_config = design_state_data['session_info']['dataset']\n",
        "    feature_config = design_state_data['feature_handling']\n",
        "    try:\n",
        "        data = load_and_preprocess_data(csv_path, feature_config)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading and preprocessing data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Target variable\n",
        "    target_config = design_state_data.get('target', {})\n",
        "    target_column = target_config.get('target', None)\n",
        "\n",
        "    if not target_column or target_column not in data.columns:\n",
        "        logging.error(\"Target column is missing or not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    target = data.pop(target_column)\n",
        "\n",
        "    # Feature reduction\n",
        "    feature_reduction_config = design_state_data.get('feature_reduction', {})\n",
        "    reduction_method = feature_reduction_config.get('feature_reduction_method', 'No Reduction')\n",
        "    try:\n",
        "        reduced_data = reduce_features(data, target, reduction_method, feature_reduction_config)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during feature reduction: {e}\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"Data preprocessing and feature reduction completed.\")\n"
      ],
      "metadata": {
        "id": "s8yjqdnIRoBK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "DYs1zeVjRpHH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map of algorithms to sklearn models\n",
        "MODEL_MAP = {\n",
        "    \"RandomForestRegressor\": RandomForestRegressor,\n",
        "    \"GBTRegressor\": GradientBoostingRegressor,\n",
        "    \"LinearRegression\": LinearRegression,\n",
        "    \"RidgeRegression\": Ridge,\n",
        "    \"LassoRegression\": Lasso,\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor,\n",
        "}"
      ],
      "metadata": {
        "id": "cWwcM2uXT-jC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models_and_params(config, prediction_type):\n",
        "    \"\"\"Parse JSON to create model objects and hyperparameter grids.\"\"\"\n",
        "    models = []\n",
        "    for algo_name, algo_config in config['algorithms'].items():\n",
        "        if algo_config['is_selected']:\n",
        "            # Check if the algorithm matches the prediction type\n",
        "            if prediction_type == \"Regression\" and \"Regressor\" in algo_name:\n",
        "                model_class = MODEL_MAP.get(algo_name)\n",
        "                if model_class:\n",
        "                    # Prepare hyperparameter grid\n",
        "                    params = {}\n",
        "                    for param_key, param_values in algo_config.items():\n",
        "                        if isinstance(param_values, list):\n",
        "                            params[param_key] = param_values\n",
        "\n",
        "                    models.append((algo_name, model_class(), params))\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "62s3Sp29UG5w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(models, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train models with GridSearchCV and evaluate on test data.\"\"\"\n",
        "    for model_name, model, param_grid in models:\n",
        "        logging.info(f\"Training model: {model_name}\")\n",
        "\n",
        "        # Create pipeline\n",
        "        pipeline = Pipeline([(\"model\", model)])\n",
        "\n",
        "        # Hyperparameter tuning\n",
        "        grid_search = GridSearchCV(pipeline, param_grid={\"model__\" + k: v for k, v in param_grid.items()}, cv=3, scoring='r2')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        best_model = grid_search.best_estimator_\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        logging.info(f\"Best Params for {model_name}: {grid_search.best_params_}\")\n",
        "        logging.info(f\"{model_name} - R2 Score: {r2_score(y_test, y_pred):.4f}, RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}\")"
      ],
      "metadata": {
        "id": "sfaDPWEPUJyX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # File paths\n",
        "    json_path = 'algoparams_from_ui.json'  # Make sure this file exists in the correct path\n",
        "    csv_path = 'iris.csv'                 # Ensure the CSV file path is correct\n",
        "\n",
        "    try:\n",
        "        # Parse JSON\n",
        "        config = parse_json(json_path)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing JSON file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Check for design_state_data key\n",
        "    if 'design_state_data' not in config:\n",
        "        logging.error(\"'design_state_data' key is missing in the JSON.\")\n",
        "        return\n",
        "\n",
        "    design_state_data = config['design_state_data']\n",
        "\n",
        "    # Ensure session_info and dataset keys exist\n",
        "    if 'session_info' not in design_state_data or 'dataset' not in design_state_data['session_info']:\n",
        "        logging.error(\"Necessary keys like 'session_info' or 'dataset' are missing in 'design_state_data'.\")\n",
        "        return\n",
        "\n",
        "    # Ensure feature_handling exists\n",
        "    if 'feature_handling' not in design_state_data:\n",
        "        logging.error(\"'feature_handling' key is missing in 'design_state_data'.\")\n",
        "        return\n",
        "\n",
        "    # Load and preprocess data\n",
        "    try:\n",
        "        feature_config = design_state_data['feature_handling']\n",
        "        data = load_and_preprocess_data(csv_path, feature_config)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading and preprocessing data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Target variable handling\n",
        "    target_config = design_state_data.get('target', {})\n",
        "    target_column = target_config.get('target', None)\n",
        "\n",
        "    if not target_column or target_column not in data.columns:\n",
        "        logging.error(\"Target column is missing or not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    target = data.pop(target_column)\n",
        "\n",
        "    # Split data\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error splitting data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Feature reduction\n",
        "    feature_reduction_config = design_state_data.get('feature_reduction', {})\n",
        "    reduction_method = feature_reduction_config.get('feature_reduction_method', 'No Reduction')\n",
        "\n",
        "    try:\n",
        "        reduced_X_train = reduce_features(X_train, y_train, reduction_method, feature_reduction_config)\n",
        "        reduced_X_test = reduce_features(X_test, y_test, reduction_method, feature_reduction_config)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during feature reduction: {e}\")\n",
        "        return\n",
        "\n",
        "    # Get prediction type and models\n",
        "    prediction_type = target_config.get('prediction_type', 'Regression')\n",
        "    try:\n",
        "        models = get_models_and_params(design_state_data, prediction_type)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error creating model objects: {e}\")\n",
        "        return\n",
        "\n",
        "    # Train and evaluate models\n",
        "    try:\n",
        "        train_and_evaluate(models, reduced_X_train, y_train, reduced_X_test, y_test)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training and evaluating models: {e}\")\n"
      ],
      "metadata": {
        "id": "HjQzUYFbUMMp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfX-TBWeU21M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}